{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0116868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "LR = 1e-3\n",
    "LR_WEIGHTS = 1e-4\n",
    "DF = 0.9\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac2f50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c974b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "weights = np.zeros(256)\n",
    "weights = np.append(weights, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfda49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bin2D:\n",
    "    def __init__(self, posx, posy, size):\n",
    "        self.posx = posx\n",
    "        self.posy = posy\n",
    "        self.size = size  # side length of the bin\n",
    "\n",
    "    def check(self, x, y):\n",
    "        if self.posx <= x < self.posx + self.size and self.posy <= y < self.posy + self.size:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Tiling2D:\n",
    "    def __init__(self, offset_x, offset_y, width, height, n_bins):\n",
    "        self.offset_x = offset_x\n",
    "        self.offset_y = offset_y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Create grid of bins for this tiling.\"\"\"\n",
    "        self.bins = []\n",
    "        cell_w = self.width / self.n_bins\n",
    "        cell_h = self.height / self.n_bins\n",
    "\n",
    "        for i in range(self.n_bins):\n",
    "            for j in range(self.n_bins):\n",
    "                x = self.offset_x + i * cell_w\n",
    "                y = self.offset_y + j * cell_h\n",
    "                self.bins.append(Bin2D(x, y, cell_w))\n",
    "\n",
    "    def check(self, x, y):\n",
    "        \"\"\"Return binary vector for this tiling.\"\"\"\n",
    "        return [b.check(x, y) for b in self.bins]\n",
    "\n",
    "\n",
    "class Tile2D:\n",
    "    def __init__(self, x_range, y_range, n_tilings, n_bins):\n",
    "        self.x_range = x_range\n",
    "        self.y_range = y_range\n",
    "        self.n_tilings = n_tilings\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Create multiple slightly offset tilings.\"\"\"\n",
    "        self.tilings = []\n",
    "        x_min, x_max = self.x_range\n",
    "        y_min, y_max = self.y_range\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "\n",
    "        for i in range(self.n_tilings):\n",
    "            # small offset for each tiling (staggered grids)\n",
    "            offset_x = x_min + (i / self.n_tilings) * (width / self.n_bins)\n",
    "            offset_y = y_min + (i / self.n_tilings) * (height / self.n_bins)\n",
    "            tiling = Tiling2D(offset_x, offset_y, width, height, self.n_bins)\n",
    "            tiling.setup()\n",
    "            self.tilings.append(tiling)\n",
    "\n",
    "    def check(self, x, y):\n",
    "        \"\"\"Return flattened binary vector of all tilings.\"\"\"\n",
    "        features = []\n",
    "        for tiling in self.tilings:\n",
    "            features.extend(tiling.check(x, y))\n",
    "        return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def x_of_s_a(s, a, tile2d, n_actions=3):\n",
    "    \"\"\"Return full state-action feature vector x(s,a).\"\"\"\n",
    "    phi = tile2d.check(*s)\n",
    "    n = len(phi)\n",
    "    x = np.zeros(n_actions * n)\n",
    "    x[a * n:(a + 1) * n] = phi\n",
    "    return x\n",
    "\n",
    "pos_range = (-2.4, 2.4)\n",
    "pol_angle_range = (-0.2095, 0.2095)\n",
    "cart_velocity = (-4, 4)\n",
    "pol_ang_vel = (-4, 4)\n",
    "\n",
    "tile1 = Tile2D(pos_range, cart_velocity, n_tilings=4, n_bins=4)\n",
    "tile2 = Tile2D(pol_angle_range, pol_ang_vel, n_tilings=4, n_bins=4)\n",
    "\n",
    "tile1.setup()\n",
    "tile2.setup()\n",
    "\n",
    "tiles = [tile1, tile2]\n",
    "\n",
    "def create_feature_vector_ntiles(obs, action):\n",
    "    global tiles\n",
    "    feat_v = []\n",
    "    feat_v.extend(x_of_s_a(obs[:2], action, tiles[0], 2))\n",
    "    feat_v.extend(x_of_s_a(obs[2:], action, tiles[1], 2))\n",
    "    return np.array(feat_v)\n",
    "\n",
    "def get_state_val(obs_vec):\n",
    "    global weights\n",
    "    return np.dot(obs_vec, weights[:-1]) + weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "426df80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_REINFORCE(epochs):\n",
    "    global model\n",
    "    global optimizer\n",
    "    global weights\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        terminated = truncated = False\n",
    "        episode = []\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            logits = model(obs_t).squeeze(0)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            old_obs = obs\n",
    "            obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode.append((old_obs, action, reward, log_prob))\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for obs, action, reward, _ in reversed(episode):\n",
    "            G = reward + DF * G\n",
    "            feature_vec = create_feature_vector_ntiles(obs, action)\n",
    "            td_error = G - get_state_val(feature_vec)\n",
    "            feature_vec = np.append(feature_vec, 1)\n",
    "            weights += LR_WEIGHTS*td_error*feature_vec\n",
    "            returns.insert(0, td_error)\n",
    "\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "        #returns = (returns - returns.mean()) / (returns.std(unbiased=False) + 1e-8)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = 0\n",
    "        for (_, _, _, log_prob), G in zip(episode, returns):\n",
    "            loss += -log_prob * G\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {ep+1}, loss={loss.item():.4f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def testing_loop_REINFORCE(epochs, nsteps):\n",
    "    global model\n",
    "    global optimizer\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        terminated = truncated = False\n",
    "\n",
    "        while not (terminated or step > nsteps):\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            logits = model(obs_t).squeeze(0)\n",
    "            dist = torch.distributions.Categorical(logits = logits)\n",
    "            action = dist.sample()\n",
    "\n",
    "            obs, _, terminated, truncated, _ = env.step(action.item())\n",
    "            step += 1\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b425d4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, loss=32.4451\n",
      "Episode 2, loss=131.1698\n",
      "Episode 3, loss=33.2898\n",
      "Episode 4, loss=23.8246\n",
      "Episode 5, loss=88.6883\n",
      "Episode 6, loss=45.7618\n",
      "Episode 7, loss=22.8177\n",
      "Episode 8, loss=99.8301\n",
      "Episode 9, loss=70.9209\n",
      "Episode 10, loss=192.2784\n",
      "Episode 11, loss=22.3662\n",
      "Episode 12, loss=116.0356\n",
      "Episode 13, loss=107.8345\n",
      "Episode 14, loss=115.2021\n",
      "Episode 15, loss=30.1538\n",
      "Episode 16, loss=79.4904\n",
      "Episode 17, loss=58.0739\n",
      "Episode 18, loss=225.3856\n",
      "Episode 19, loss=222.1423\n",
      "Episode 20, loss=42.1475\n",
      "Episode 21, loss=201.6336\n",
      "Episode 22, loss=70.5290\n",
      "Episode 23, loss=34.1335\n",
      "Episode 24, loss=28.5818\n",
      "Episode 25, loss=55.2704\n",
      "Episode 26, loss=64.6542\n",
      "Episode 27, loss=43.3510\n",
      "Episode 28, loss=275.1664\n",
      "Episode 29, loss=67.9917\n",
      "Episode 30, loss=115.7313\n",
      "Episode 31, loss=247.5483\n",
      "Episode 32, loss=40.7236\n",
      "Episode 33, loss=68.3396\n",
      "Episode 34, loss=18.0804\n",
      "Episode 35, loss=57.3896\n",
      "Episode 36, loss=42.6932\n",
      "Episode 37, loss=437.2654\n",
      "Episode 38, loss=149.1047\n",
      "Episode 39, loss=33.8935\n",
      "Episode 40, loss=32.2207\n",
      "Episode 41, loss=165.3138\n",
      "Episode 42, loss=287.4219\n",
      "Episode 43, loss=81.0582\n",
      "Episode 44, loss=49.6564\n",
      "Episode 45, loss=86.1943\n",
      "Episode 46, loss=188.1706\n",
      "Episode 47, loss=20.4264\n",
      "Episode 48, loss=45.6444\n",
      "Episode 49, loss=27.2146\n",
      "Episode 50, loss=204.1371\n",
      "Episode 51, loss=106.6461\n",
      "Episode 52, loss=45.5907\n",
      "Episode 53, loss=145.8227\n",
      "Episode 54, loss=373.1304\n",
      "Episode 55, loss=542.7914\n",
      "Episode 56, loss=231.2369\n",
      "Episode 57, loss=19.4676\n",
      "Episode 58, loss=34.5615\n",
      "Episode 59, loss=50.6582\n",
      "Episode 60, loss=43.4926\n",
      "Episode 61, loss=26.1103\n",
      "Episode 62, loss=106.4661\n",
      "Episode 63, loss=134.7763\n",
      "Episode 64, loss=233.8214\n",
      "Episode 65, loss=244.2926\n",
      "Episode 66, loss=143.3887\n",
      "Episode 67, loss=21.3311\n",
      "Episode 68, loss=53.1217\n",
      "Episode 69, loss=43.8572\n",
      "Episode 70, loss=210.3317\n",
      "Episode 71, loss=260.7402\n",
      "Episode 72, loss=245.3285\n",
      "Episode 73, loss=102.5324\n",
      "Episode 74, loss=31.6976\n",
      "Episode 75, loss=97.9726\n",
      "Episode 76, loss=101.2548\n",
      "Episode 77, loss=154.6663\n",
      "Episode 78, loss=209.4297\n",
      "Episode 79, loss=228.3450\n",
      "Episode 80, loss=81.2985\n",
      "Episode 81, loss=36.6589\n",
      "Episode 82, loss=263.7227\n",
      "Episode 83, loss=34.6392\n",
      "Episode 84, loss=78.8114\n",
      "Episode 85, loss=439.5879\n",
      "Episode 86, loss=111.2144\n",
      "Episode 87, loss=38.7127\n",
      "Episode 88, loss=127.9415\n",
      "Episode 89, loss=70.6297\n",
      "Episode 90, loss=71.7850\n",
      "Episode 91, loss=76.8761\n",
      "Episode 92, loss=269.3886\n",
      "Episode 93, loss=6.7015\n",
      "Episode 94, loss=127.7727\n",
      "Episode 95, loss=156.8301\n",
      "Episode 96, loss=25.4421\n",
      "Episode 97, loss=105.6214\n",
      "Episode 98, loss=42.4538\n",
      "Episode 99, loss=64.9468\n",
      "Episode 100, loss=15.0546\n",
      "Episode 101, loss=275.3451\n",
      "Episode 102, loss=21.9952\n",
      "Episode 103, loss=16.2953\n",
      "Episode 104, loss=37.2816\n",
      "Episode 105, loss=19.7637\n",
      "Episode 106, loss=47.7017\n",
      "Episode 107, loss=23.3855\n",
      "Episode 108, loss=350.2106\n",
      "Episode 109, loss=10.6145\n",
      "Episode 110, loss=43.5544\n",
      "Episode 111, loss=60.6912\n",
      "Episode 112, loss=82.1013\n",
      "Episode 113, loss=33.5068\n",
      "Episode 114, loss=22.6713\n",
      "Episode 115, loss=189.2121\n",
      "Episode 116, loss=31.5391\n",
      "Episode 117, loss=22.7772\n",
      "Episode 118, loss=41.4082\n",
      "Episode 119, loss=326.6973\n",
      "Episode 120, loss=38.2510\n",
      "Episode 121, loss=152.0578\n",
      "Episode 122, loss=42.4425\n",
      "Episode 123, loss=106.1262\n",
      "Episode 124, loss=6.2335\n",
      "Episode 125, loss=28.6100\n",
      "Episode 126, loss=96.7843\n",
      "Episode 127, loss=35.9412\n",
      "Episode 128, loss=62.9500\n",
      "Episode 129, loss=303.8663\n",
      "Episode 130, loss=504.5517\n",
      "Episode 131, loss=285.7766\n",
      "Episode 132, loss=92.3813\n",
      "Episode 133, loss=52.8796\n",
      "Episode 134, loss=162.9787\n",
      "Episode 135, loss=324.5944\n",
      "Episode 136, loss=25.0527\n",
      "Episode 137, loss=29.1516\n",
      "Episode 138, loss=302.0409\n",
      "Episode 139, loss=36.0384\n",
      "Episode 140, loss=19.1810\n",
      "Episode 141, loss=62.9955\n",
      "Episode 142, loss=152.6238\n",
      "Episode 143, loss=60.5846\n",
      "Episode 144, loss=-5.9899\n",
      "Episode 145, loss=145.3315\n",
      "Episode 146, loss=59.6663\n",
      "Episode 147, loss=94.7016\n",
      "Episode 148, loss=91.3453\n",
      "Episode 149, loss=49.3682\n",
      "Episode 150, loss=80.1121\n",
      "Episode 151, loss=-2.8135\n",
      "Episode 152, loss=425.7444\n",
      "Episode 153, loss=29.5499\n",
      "Episode 154, loss=7.7941\n",
      "Episode 155, loss=10.8281\n",
      "Episode 156, loss=-2.2910\n",
      "Episode 157, loss=104.4624\n",
      "Episode 158, loss=266.7616\n",
      "Episode 159, loss=56.1081\n",
      "Episode 160, loss=150.3105\n",
      "Episode 161, loss=78.8598\n",
      "Episode 162, loss=-0.1341\n",
      "Episode 163, loss=119.3682\n",
      "Episode 164, loss=56.2132\n",
      "Episode 165, loss=185.4962\n",
      "Episode 166, loss=30.3065\n",
      "Episode 167, loss=67.1711\n",
      "Episode 168, loss=58.8966\n",
      "Episode 169, loss=67.9256\n",
      "Episode 170, loss=59.1108\n",
      "Episode 171, loss=0.7696\n",
      "Episode 172, loss=18.6579\n",
      "Episode 173, loss=17.0748\n",
      "Episode 174, loss=313.8665\n",
      "Episode 175, loss=136.0252\n",
      "Episode 176, loss=30.8719\n",
      "Episode 177, loss=3.7029\n",
      "Episode 178, loss=44.5179\n",
      "Episode 179, loss=104.9299\n",
      "Episode 180, loss=21.6912\n",
      "Episode 181, loss=28.7556\n",
      "Episode 182, loss=229.7250\n",
      "Episode 183, loss=121.2085\n",
      "Episode 184, loss=21.6938\n",
      "Episode 185, loss=74.2883\n",
      "Episode 186, loss=154.9133\n",
      "Episode 187, loss=27.4432\n",
      "Episode 188, loss=38.1935\n",
      "Episode 189, loss=190.4084\n",
      "Episode 190, loss=55.5115\n",
      "Episode 191, loss=94.9112\n",
      "Episode 192, loss=183.7389\n",
      "Episode 193, loss=40.5413\n",
      "Episode 194, loss=106.7859\n",
      "Episode 195, loss=166.6303\n",
      "Episode 196, loss=76.3108\n",
      "Episode 197, loss=145.4348\n",
      "Episode 198, loss=19.7706\n",
      "Episode 199, loss=20.4789\n",
      "Episode 200, loss=7.3738\n",
      "Episode 201, loss=193.3069\n",
      "Episode 202, loss=33.2896\n",
      "Episode 203, loss=-8.0988\n",
      "Episode 204, loss=145.8556\n",
      "Episode 205, loss=-12.7266\n",
      "Episode 206, loss=79.2874\n",
      "Episode 207, loss=228.8216\n",
      "Episode 208, loss=-2.2916\n",
      "Episode 209, loss=6.0802\n",
      "Episode 210, loss=35.2779\n",
      "Episode 211, loss=88.8546\n",
      "Episode 212, loss=34.3372\n",
      "Episode 213, loss=154.0716\n",
      "Episode 214, loss=14.8305\n",
      "Episode 215, loss=25.5398\n",
      "Episode 216, loss=462.7521\n",
      "Episode 217, loss=18.2604\n",
      "Episode 218, loss=47.3572\n",
      "Episode 219, loss=93.5996\n",
      "Episode 220, loss=13.7653\n",
      "Episode 221, loss=-15.1695\n",
      "Episode 222, loss=39.7712\n",
      "Episode 223, loss=-13.9534\n",
      "Episode 224, loss=61.9185\n",
      "Episode 225, loss=1.7202\n",
      "Episode 226, loss=180.8651\n",
      "Episode 227, loss=142.5383\n",
      "Episode 228, loss=152.4301\n",
      "Episode 229, loss=5.4750\n",
      "Episode 230, loss=280.2572\n",
      "Episode 231, loss=35.4859\n",
      "Episode 232, loss=30.3922\n",
      "Episode 233, loss=-2.3722\n",
      "Episode 234, loss=28.8684\n",
      "Episode 235, loss=10.1895\n",
      "Episode 236, loss=35.5543\n",
      "Episode 237, loss=63.8339\n",
      "Episode 238, loss=274.8454\n",
      "Episode 239, loss=140.2713\n",
      "Episode 240, loss=51.9581\n",
      "Episode 241, loss=20.0716\n",
      "Episode 242, loss=43.8734\n",
      "Episode 243, loss=44.8459\n",
      "Episode 244, loss=190.0883\n",
      "Episode 245, loss=15.0396\n",
      "Episode 246, loss=263.5604\n",
      "Episode 247, loss=382.4201\n",
      "Episode 248, loss=300.6784\n",
      "Episode 249, loss=242.5965\n",
      "Episode 250, loss=55.6864\n",
      "Episode 251, loss=253.5624\n",
      "Episode 252, loss=-12.7915\n",
      "Episode 253, loss=10.0723\n",
      "Episode 254, loss=-20.4703\n",
      "Episode 255, loss=475.5784\n",
      "Episode 256, loss=205.4352\n",
      "Episode 257, loss=145.9166\n",
      "Episode 258, loss=56.4661\n",
      "Episode 259, loss=170.9460\n",
      "Episode 260, loss=301.7777\n",
      "Episode 261, loss=282.1287\n",
      "Episode 262, loss=-28.5668\n",
      "Episode 263, loss=569.1773\n",
      "Episode 264, loss=297.2127\n",
      "Episode 265, loss=300.8640\n",
      "Episode 266, loss=47.0946\n",
      "Episode 267, loss=-27.4782\n",
      "Episode 268, loss=-41.6853\n",
      "Episode 269, loss=311.0967\n",
      "Episode 270, loss=29.2890\n",
      "Episode 271, loss=251.3487\n",
      "Episode 272, loss=227.8746\n",
      "Episode 273, loss=-5.2256\n",
      "Episode 274, loss=117.5866\n",
      "Episode 275, loss=-8.6575\n",
      "Episode 276, loss=325.5205\n",
      "Episode 277, loss=132.4707\n",
      "Episode 278, loss=122.1476\n",
      "Episode 279, loss=137.0717\n",
      "Episode 280, loss=-38.4333\n",
      "Episode 281, loss=-26.0111\n",
      "Episode 282, loss=16.8659\n",
      "Episode 283, loss=116.4764\n",
      "Episode 284, loss=245.1216\n",
      "Episode 285, loss=157.7509\n",
      "Episode 286, loss=347.1508\n",
      "Episode 287, loss=-31.2585\n",
      "Episode 288, loss=-21.6422\n",
      "Episode 289, loss=-39.6863\n",
      "Episode 290, loss=30.3500\n",
      "Episode 291, loss=13.5575\n",
      "Episode 292, loss=216.4075\n",
      "Episode 293, loss=142.5432\n",
      "Episode 294, loss=210.6046\n",
      "Episode 295, loss=212.2895\n",
      "Episode 296, loss=110.3211\n",
      "Episode 297, loss=261.8939\n",
      "Episode 298, loss=28.8320\n",
      "Episode 299, loss=104.8405\n",
      "Episode 300, loss=132.2339\n",
      "Episode 301, loss=108.4966\n",
      "Episode 302, loss=68.1576\n",
      "Episode 303, loss=154.9692\n",
      "Episode 304, loss=-40.9908\n",
      "Episode 305, loss=122.8437\n",
      "Episode 306, loss=131.5839\n",
      "Episode 307, loss=72.6950\n",
      "Episode 308, loss=103.5225\n",
      "Episode 309, loss=-11.6471\n",
      "Episode 310, loss=102.0075\n",
      "Episode 311, loss=-67.2320\n",
      "Episode 312, loss=64.3546\n",
      "Episode 313, loss=35.1006\n",
      "Episode 314, loss=101.9685\n",
      "Episode 315, loss=259.0941\n",
      "Episode 316, loss=-3.5079\n",
      "Episode 317, loss=1.7203\n",
      "Episode 318, loss=-41.2324\n",
      "Episode 319, loss=5.8492\n",
      "Episode 320, loss=45.3651\n",
      "Episode 321, loss=614.4963\n",
      "Episode 322, loss=222.0275\n",
      "Episode 323, loss=2.3429\n",
      "Episode 324, loss=436.0453\n",
      "Episode 325, loss=126.9605\n",
      "Episode 326, loss=-45.0201\n",
      "Episode 327, loss=111.0106\n",
      "Episode 328, loss=186.8769\n",
      "Episode 329, loss=168.5270\n",
      "Episode 330, loss=131.7568\n",
      "Episode 331, loss=144.7109\n",
      "Episode 332, loss=-40.8273\n",
      "Episode 333, loss=130.5700\n",
      "Episode 334, loss=-23.6619\n",
      "Episode 335, loss=10.0292\n",
      "Episode 336, loss=161.8498\n",
      "Episode 337, loss=120.4671\n",
      "Episode 338, loss=185.0467\n",
      "Episode 339, loss=143.7735\n",
      "Episode 340, loss=-45.9831\n",
      "Episode 341, loss=-65.7475\n",
      "Episode 342, loss=91.0167\n",
      "Episode 343, loss=54.5992\n",
      "Episode 344, loss=135.4888\n",
      "Episode 345, loss=242.7725\n",
      "Episode 346, loss=-55.3465\n",
      "Episode 347, loss=147.6227\n",
      "Episode 348, loss=187.3159\n",
      "Episode 349, loss=169.6374\n",
      "Episode 350, loss=211.2501\n",
      "Episode 351, loss=50.6970\n",
      "Episode 352, loss=-73.3131\n",
      "Episode 353, loss=129.6903\n",
      "Episode 354, loss=53.4763\n",
      "Episode 355, loss=251.2422\n",
      "Episode 356, loss=180.3906\n",
      "Episode 357, loss=45.7195\n",
      "Episode 358, loss=39.3306\n",
      "Episode 359, loss=51.1494\n",
      "Episode 360, loss=94.5597\n",
      "Episode 361, loss=57.7697\n",
      "Episode 362, loss=35.9756\n",
      "Episode 363, loss=202.3279\n",
      "Episode 364, loss=-58.1537\n",
      "Episode 365, loss=-26.1883\n",
      "Episode 366, loss=-52.6120\n",
      "Episode 367, loss=48.1178\n",
      "Episode 368, loss=22.6380\n",
      "Episode 369, loss=108.0914\n",
      "Episode 370, loss=17.3900\n",
      "Episode 371, loss=-25.7737\n",
      "Episode 372, loss=156.6010\n",
      "Episode 373, loss=100.7310\n",
      "Episode 374, loss=64.2823\n",
      "Episode 375, loss=81.8656\n",
      "Episode 376, loss=31.3443\n",
      "Episode 377, loss=20.3659\n",
      "Episode 378, loss=75.0810\n",
      "Episode 379, loss=-52.2539\n",
      "Episode 380, loss=163.7164\n",
      "Episode 381, loss=-31.6147\n",
      "Episode 382, loss=128.6591\n",
      "Episode 383, loss=140.4769\n",
      "Episode 384, loss=52.2923\n",
      "Episode 385, loss=27.2902\n",
      "Episode 386, loss=-107.8875\n",
      "Episode 387, loss=65.7994\n",
      "Episode 388, loss=110.1976\n",
      "Episode 389, loss=31.7813\n",
      "Episode 390, loss=103.2901\n",
      "Episode 391, loss=69.1228\n",
      "Episode 392, loss=-50.7740\n",
      "Episode 393, loss=331.7005\n",
      "Episode 394, loss=209.8555\n",
      "Episode 395, loss=96.6910\n",
      "Episode 396, loss=-47.0002\n",
      "Episode 397, loss=36.6730\n",
      "Episode 398, loss=193.0652\n",
      "Episode 399, loss=-81.8534\n",
      "Episode 400, loss=31.0115\n",
      "Episode 401, loss=5.7490\n",
      "Episode 402, loss=109.0853\n",
      "Episode 403, loss=80.9457\n",
      "Episode 404, loss=97.4814\n",
      "Episode 405, loss=-43.8712\n",
      "Episode 406, loss=-19.5430\n",
      "Episode 407, loss=20.1426\n",
      "Episode 408, loss=30.7204\n",
      "Episode 409, loss=96.3000\n",
      "Episode 410, loss=46.3658\n",
      "Episode 411, loss=93.1312\n",
      "Episode 412, loss=13.8775\n",
      "Episode 413, loss=6.6145\n",
      "Episode 414, loss=94.8996\n",
      "Episode 415, loss=191.8514\n",
      "Episode 416, loss=111.7461\n",
      "Episode 417, loss=32.8157\n",
      "Episode 418, loss=51.3571\n",
      "Episode 419, loss=79.5406\n",
      "Episode 420, loss=164.6826\n",
      "Episode 421, loss=61.7104\n",
      "Episode 422, loss=101.6204\n",
      "Episode 423, loss=147.9231\n",
      "Episode 424, loss=-53.6950\n",
      "Episode 425, loss=171.2667\n",
      "Episode 426, loss=14.4548\n",
      "Episode 427, loss=48.6137\n",
      "Episode 428, loss=71.8122\n",
      "Episode 429, loss=-81.4051\n",
      "Episode 430, loss=12.2382\n",
      "Episode 431, loss=21.1316\n",
      "Episode 432, loss=-10.9759\n",
      "Episode 433, loss=-0.1416\n",
      "Episode 434, loss=-83.4632\n",
      "Episode 435, loss=29.0001\n",
      "Episode 436, loss=-41.2980\n",
      "Episode 437, loss=-113.2148\n",
      "Episode 438, loss=2.2454\n",
      "Episode 439, loss=-13.3862\n",
      "Episode 440, loss=179.1620\n",
      "Episode 441, loss=9.6940\n",
      "Episode 442, loss=-7.8939\n",
      "Episode 443, loss=35.0304\n",
      "Episode 444, loss=79.0263\n",
      "Episode 445, loss=12.0977\n",
      "Episode 446, loss=-19.8916\n",
      "Episode 447, loss=-70.4325\n",
      "Episode 448, loss=189.1987\n",
      "Episode 449, loss=-86.9889\n",
      "Episode 450, loss=92.9263\n",
      "Episode 451, loss=111.4779\n",
      "Episode 452, loss=5.0640\n",
      "Episode 453, loss=-93.8862\n",
      "Episode 454, loss=10.3382\n",
      "Episode 455, loss=-68.1434\n",
      "Episode 456, loss=-34.7306\n",
      "Episode 457, loss=-41.5921\n",
      "Episode 458, loss=52.5097\n",
      "Episode 459, loss=-37.9679\n",
      "Episode 460, loss=183.5081\n",
      "Episode 461, loss=-44.6641\n",
      "Episode 462, loss=-55.8644\n",
      "Episode 463, loss=112.5264\n",
      "Episode 464, loss=-58.5420\n",
      "Episode 465, loss=57.9660\n",
      "Episode 466, loss=45.1322\n",
      "Episode 467, loss=-54.1461\n",
      "Episode 468, loss=-4.8059\n",
      "Episode 469, loss=-93.6459\n",
      "Episode 470, loss=-12.0039\n",
      "Episode 471, loss=46.4925\n",
      "Episode 472, loss=26.9441\n",
      "Episode 473, loss=10.7133\n",
      "Episode 474, loss=70.4867\n",
      "Episode 475, loss=50.9456\n",
      "Episode 476, loss=-13.5489\n",
      "Episode 477, loss=14.4078\n",
      "Episode 478, loss=36.3781\n",
      "Episode 479, loss=82.6576\n",
      "Episode 480, loss=60.1025\n",
      "Episode 481, loss=-36.5030\n",
      "Episode 482, loss=32.8345\n",
      "Episode 483, loss=90.7211\n",
      "Episode 484, loss=-54.5819\n",
      "Episode 485, loss=65.4183\n",
      "Episode 486, loss=-34.4326\n",
      "Episode 487, loss=43.6609\n",
      "Episode 488, loss=-59.6827\n",
      "Episode 489, loss=94.0247\n",
      "Episode 490, loss=-123.5619\n",
      "Episode 491, loss=96.2593\n",
      "Episode 492, loss=63.7679\n",
      "Episode 493, loss=158.7694\n",
      "Episode 494, loss=34.0152\n",
      "Episode 495, loss=38.8571\n",
      "Episode 496, loss=-64.0093\n",
      "Episode 497, loss=23.3616\n",
      "Episode 498, loss=-22.3036\n",
      "Episode 499, loss=73.7138\n",
      "Episode 500, loss=35.5540\n",
      "Episode 501, loss=-62.3505\n",
      "Episode 502, loss=-70.6955\n",
      "Episode 503, loss=60.1671\n",
      "Episode 504, loss=34.4708\n",
      "Episode 505, loss=-43.6342\n",
      "Episode 506, loss=-59.4779\n",
      "Episode 507, loss=-44.5268\n",
      "Episode 508, loss=88.2881\n",
      "Episode 509, loss=-16.9980\n",
      "Episode 510, loss=-24.5312\n",
      "Episode 511, loss=45.7034\n",
      "Episode 512, loss=-70.0584\n",
      "Episode 513, loss=74.4421\n",
      "Episode 514, loss=-86.3603\n",
      "Episode 515, loss=147.1033\n",
      "Episode 516, loss=16.6836\n",
      "Episode 517, loss=28.7444\n",
      "Episode 518, loss=36.1927\n",
      "Episode 519, loss=43.7444\n",
      "Episode 520, loss=-68.9070\n",
      "Episode 521, loss=19.6417\n",
      "Episode 522, loss=89.1342\n",
      "Episode 523, loss=-62.9064\n",
      "Episode 524, loss=135.7305\n",
      "Episode 525, loss=-59.4417\n",
      "Episode 526, loss=130.1708\n",
      "Episode 527, loss=-99.4066\n",
      "Episode 528, loss=52.4905\n",
      "Episode 529, loss=-12.0854\n",
      "Episode 530, loss=-5.7925\n",
      "Episode 531, loss=-90.9366\n",
      "Episode 532, loss=99.2489\n",
      "Episode 533, loss=2.1812\n",
      "Episode 534, loss=-41.1480\n",
      "Episode 535, loss=-85.9679\n",
      "Episode 536, loss=-46.7752\n",
      "Episode 537, loss=117.0473\n",
      "Episode 538, loss=21.9300\n",
      "Episode 539, loss=61.1956\n",
      "Episode 540, loss=34.1051\n",
      "Episode 541, loss=74.1214\n",
      "Episode 542, loss=29.7603\n",
      "Episode 543, loss=79.3372\n",
      "Episode 544, loss=-36.1634\n",
      "Episode 545, loss=5.1092\n",
      "Episode 546, loss=126.3354\n",
      "Episode 547, loss=-104.7735\n",
      "Episode 548, loss=9.8027\n",
      "Episode 549, loss=7.3651\n",
      "Episode 550, loss=41.3486\n",
      "Episode 551, loss=-85.1909\n",
      "Episode 552, loss=88.5174\n",
      "Episode 553, loss=-61.4162\n",
      "Episode 554, loss=-26.2047\n",
      "Episode 555, loss=31.8614\n",
      "Episode 556, loss=43.9873\n",
      "Episode 557, loss=76.6981\n",
      "Episode 558, loss=-27.2933\n",
      "Episode 559, loss=-75.3837\n",
      "Episode 560, loss=-68.3164\n",
      "Episode 561, loss=84.1457\n",
      "Episode 562, loss=30.5234\n",
      "Episode 563, loss=-61.6417\n",
      "Episode 564, loss=38.4282\n",
      "Episode 565, loss=65.3184\n",
      "Episode 566, loss=-7.3494\n",
      "Episode 567, loss=-44.1527\n",
      "Episode 568, loss=45.7104\n",
      "Episode 569, loss=29.6636\n",
      "Episode 570, loss=-38.8218\n",
      "Episode 571, loss=-13.4225\n",
      "Episode 572, loss=116.5109\n",
      "Episode 573, loss=45.8017\n",
      "Episode 574, loss=54.1243\n",
      "Episode 575, loss=-50.7325\n",
      "Episode 576, loss=-36.0124\n",
      "Episode 577, loss=27.5982\n",
      "Episode 578, loss=27.8834\n",
      "Episode 579, loss=3.9468\n",
      "Episode 580, loss=62.2358\n",
      "Episode 581, loss=30.2586\n",
      "Episode 582, loss=24.6694\n",
      "Episode 583, loss=25.7291\n",
      "Episode 584, loss=34.7683\n",
      "Episode 585, loss=16.5350\n",
      "Episode 586, loss=36.0692\n",
      "Episode 587, loss=2.5709\n",
      "Episode 588, loss=-28.3307\n",
      "Episode 589, loss=8.5669\n",
      "Episode 590, loss=44.7612\n",
      "Episode 591, loss=-36.2700\n",
      "Episode 592, loss=-21.1403\n",
      "Episode 593, loss=-71.3343\n",
      "Episode 594, loss=-52.7346\n",
      "Episode 595, loss=41.6333\n",
      "Episode 596, loss=-46.8533\n",
      "Episode 597, loss=-12.0866\n",
      "Episode 598, loss=122.9286\n",
      "Episode 599, loss=72.9646\n",
      "Episode 600, loss=5.1989\n",
      "Episode 601, loss=-32.2394\n",
      "Episode 602, loss=-63.1517\n",
      "Episode 603, loss=-9.1651\n",
      "Episode 604, loss=119.4164\n",
      "Episode 605, loss=-51.7259\n",
      "Episode 606, loss=-66.3947\n",
      "Episode 607, loss=-40.8712\n",
      "Episode 608, loss=84.6938\n",
      "Episode 609, loss=-18.3150\n",
      "Episode 610, loss=-67.2150\n",
      "Episode 611, loss=66.8647\n",
      "Episode 612, loss=-68.9583\n",
      "Episode 613, loss=-74.9179\n",
      "Episode 614, loss=-40.7593\n",
      "Episode 615, loss=-52.3074\n",
      "Episode 616, loss=-20.1420\n",
      "Episode 617, loss=-21.0057\n",
      "Episode 618, loss=16.0249\n",
      "Episode 619, loss=-47.1199\n",
      "Episode 620, loss=-55.3761\n",
      "Episode 621, loss=9.1744\n",
      "Episode 622, loss=12.3767\n",
      "Episode 623, loss=-46.6973\n",
      "Episode 624, loss=-62.0575\n",
      "Episode 625, loss=38.2233\n",
      "Episode 626, loss=-26.3226\n",
      "Episode 627, loss=19.7466\n",
      "Episode 628, loss=-46.9281\n",
      "Episode 629, loss=56.5926\n",
      "Episode 630, loss=31.7458\n",
      "Episode 631, loss=-7.3922\n",
      "Episode 632, loss=-52.3938\n",
      "Episode 633, loss=-28.6584\n",
      "Episode 634, loss=121.3393\n",
      "Episode 635, loss=25.3005\n",
      "Episode 636, loss=132.2963\n",
      "Episode 637, loss=7.7427\n",
      "Episode 638, loss=58.4938\n",
      "Episode 639, loss=1.9446\n",
      "Episode 640, loss=20.0341\n",
      "Episode 641, loss=24.2631\n",
      "Episode 642, loss=133.3394\n",
      "Episode 643, loss=-46.9221\n",
      "Episode 644, loss=101.4813\n",
      "Episode 645, loss=52.7978\n",
      "Episode 646, loss=37.4303\n",
      "Episode 647, loss=-13.3864\n",
      "Episode 648, loss=-13.0358\n",
      "Episode 649, loss=60.6488\n",
      "Episode 650, loss=-52.2054\n",
      "Episode 651, loss=-3.6102\n",
      "Episode 652, loss=-46.8696\n",
      "Episode 653, loss=-53.4443\n",
      "Episode 654, loss=23.0256\n",
      "Episode 655, loss=-26.8599\n",
      "Episode 656, loss=12.1272\n",
      "Episode 657, loss=-31.3564\n",
      "Episode 658, loss=-1.0253\n",
      "Episode 659, loss=-73.4922\n",
      "Episode 660, loss=31.0301\n",
      "Episode 661, loss=-46.3194\n",
      "Episode 662, loss=-45.5720\n",
      "Episode 663, loss=-29.0521\n",
      "Episode 664, loss=12.1942\n",
      "Episode 665, loss=-1.6667\n",
      "Episode 666, loss=24.1142\n",
      "Episode 667, loss=7.5958\n",
      "Episode 668, loss=10.0162\n",
      "Episode 669, loss=-57.4757\n",
      "Episode 670, loss=-75.1409\n",
      "Episode 671, loss=32.5050\n",
      "Episode 672, loss=25.1463\n",
      "Episode 673, loss=-5.0477\n",
      "Episode 674, loss=79.3833\n",
      "Episode 675, loss=-32.7368\n",
      "Episode 676, loss=28.7701\n",
      "Episode 677, loss=4.0081\n",
      "Episode 678, loss=-56.4163\n",
      "Episode 679, loss=22.9142\n",
      "Episode 680, loss=4.2757\n",
      "Episode 681, loss=24.6242\n",
      "Episode 682, loss=63.6976\n",
      "Episode 683, loss=19.3477\n",
      "Episode 684, loss=-1.9266\n",
      "Episode 685, loss=53.3165\n",
      "Episode 686, loss=6.5518\n",
      "Episode 687, loss=-58.7273\n",
      "Episode 688, loss=61.6100\n",
      "Episode 689, loss=-36.6981\n",
      "Episode 690, loss=49.8555\n",
      "Episode 691, loss=-12.7583\n",
      "Episode 692, loss=-17.3103\n",
      "Episode 693, loss=-12.8174\n",
      "Episode 694, loss=-34.6760\n",
      "Episode 695, loss=-18.7073\n",
      "Episode 696, loss=-11.6939\n",
      "Episode 697, loss=-0.9979\n",
      "Episode 698, loss=-34.6436\n",
      "Episode 699, loss=11.4754\n",
      "Episode 700, loss=30.2021\n",
      "Episode 701, loss=-40.4221\n",
      "Episode 702, loss=47.3380\n",
      "Episode 703, loss=-28.4679\n",
      "Episode 704, loss=32.6429\n",
      "Episode 705, loss=16.8876\n",
      "Episode 706, loss=-3.1034\n",
      "Episode 707, loss=65.2046\n",
      "Episode 708, loss=0.4936\n",
      "Episode 709, loss=-13.1645\n",
      "Episode 710, loss=-13.9490\n",
      "Episode 711, loss=-4.4764\n",
      "Episode 712, loss=119.5427\n",
      "Episode 713, loss=22.1868\n",
      "Episode 714, loss=-20.5968\n",
      "Episode 715, loss=7.1115\n",
      "Episode 716, loss=-15.6888\n",
      "Episode 717, loss=-1.1093\n",
      "Episode 718, loss=26.6032\n",
      "Episode 719, loss=-37.1602\n",
      "Episode 720, loss=76.2827\n",
      "Episode 721, loss=-37.7408\n",
      "Episode 722, loss=20.2601\n",
      "Episode 723, loss=48.4947\n",
      "Episode 724, loss=13.0702\n",
      "Episode 725, loss=-48.6167\n",
      "Episode 726, loss=-69.6559\n",
      "Episode 727, loss=-23.9544\n",
      "Episode 728, loss=-13.5698\n",
      "Episode 729, loss=3.8915\n",
      "Episode 730, loss=-6.7899\n",
      "Episode 731, loss=2.8499\n",
      "Episode 732, loss=0.4152\n",
      "Episode 733, loss=-25.2351\n",
      "Episode 734, loss=159.2639\n",
      "Episode 735, loss=4.2685\n",
      "Episode 736, loss=-34.7877\n",
      "Episode 737, loss=-19.3032\n",
      "Episode 738, loss=-35.1678\n",
      "Episode 739, loss=-28.7979\n",
      "Episode 740, loss=-32.6961\n",
      "Episode 741, loss=6.0403\n",
      "Episode 742, loss=23.3309\n",
      "Episode 743, loss=-30.0835\n",
      "Episode 744, loss=13.1309\n",
      "Episode 745, loss=-50.3598\n",
      "Episode 746, loss=15.8613\n",
      "Episode 747, loss=19.4082\n",
      "Episode 748, loss=11.8894\n",
      "Episode 749, loss=-7.9439\n",
      "Episode 750, loss=45.7195\n",
      "Episode 751, loss=36.0699\n",
      "Episode 752, loss=-28.5571\n",
      "Episode 753, loss=-27.7468\n",
      "Episode 754, loss=-19.7532\n",
      "Episode 755, loss=16.8522\n",
      "Episode 756, loss=-60.2902\n",
      "Episode 757, loss=-26.3031\n",
      "Episode 758, loss=-3.8874\n",
      "Episode 759, loss=111.6109\n",
      "Episode 760, loss=-34.8189\n",
      "Episode 761, loss=2.8607\n",
      "Episode 762, loss=21.6679\n",
      "Episode 763, loss=18.6799\n",
      "Episode 764, loss=-43.2231\n",
      "Episode 765, loss=60.5215\n",
      "Episode 766, loss=-23.5232\n",
      "Episode 767, loss=-25.6390\n",
      "Episode 768, loss=4.8864\n",
      "Episode 769, loss=-0.6275\n",
      "Episode 770, loss=-24.2577\n",
      "Episode 771, loss=-19.1015\n",
      "Episode 772, loss=65.9034\n",
      "Episode 773, loss=-13.1824\n",
      "Episode 774, loss=17.0860\n",
      "Episode 775, loss=88.6653\n",
      "Episode 776, loss=30.3415\n",
      "Episode 777, loss=-29.2973\n",
      "Episode 778, loss=19.2067\n",
      "Episode 779, loss=63.7801\n",
      "Episode 780, loss=-25.4222\n",
      "Episode 781, loss=-46.3501\n",
      "Episode 782, loss=-15.6987\n",
      "Episode 783, loss=-22.0632\n",
      "Episode 784, loss=-73.9777\n",
      "Episode 785, loss=70.8397\n",
      "Episode 786, loss=6.1052\n",
      "Episode 787, loss=-5.0889\n",
      "Episode 788, loss=-45.9734\n",
      "Episode 789, loss=88.2051\n",
      "Episode 790, loss=6.8239\n",
      "Episode 791, loss=-21.1859\n",
      "Episode 792, loss=-4.6650\n",
      "Episode 793, loss=-3.2838\n",
      "Episode 794, loss=26.1729\n",
      "Episode 795, loss=7.0506\n",
      "Episode 796, loss=89.2947\n",
      "Episode 797, loss=4.2556\n",
      "Episode 798, loss=77.5465\n",
      "Episode 799, loss=4.0060\n",
      "Episode 800, loss=57.8631\n"
     ]
    }
   ],
   "source": [
    "training_loop_REINFORCE(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9432123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loop_REINFORCE(1, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b56e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
