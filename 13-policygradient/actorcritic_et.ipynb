{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a90b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Normal\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e8d66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor network\n",
    "class ActorNeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim) # Distribution mean\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mu = torch.tanh(self.mu(x))\n",
    "        log_std = torch.clamp(self.log_std, -10, 10)  # safe range\n",
    "        std = log_std.exp()\n",
    "        return mu, std\n",
    "    \n",
    "    def get_dist(self, state):\n",
    "        mu, std = self(state)\n",
    "        return Normal(mu, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1023fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Critic network\n",
    "\n",
    "class CriticNeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59737e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedDoublePendulum-v5\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "env.close()\n",
    "\n",
    "actor = ActorNeuralNetwork(state_dim, action_dim)\n",
    "critic = CriticNeuralNetwork(state_dim)\n",
    "\n",
    "# actor_optimizer = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "# critic_optimizer = torch.optim.Adam(critic.parameters(), lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9238c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor-Critic w/ Eligibility Traces\n",
    "\n",
    "def training_loop_ACWET(epochs, lr_theta, lr_weight, discount, lambda_et, discount_lr):\n",
    "    global actor, critic\n",
    "\n",
    "    env = gym.make(\"InvertedDoublePendulum-v5\")\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        actor_traces = [torch.zeros_like(p.data) for p in actor.parameters()]\n",
    "        critic_traces = [torch.zeros_like(p.data) for p in critic.parameters()]\n",
    "        terminated = truncated = False\n",
    "        obs, _ = env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            actor_dist = actor.get_dist(obs)\n",
    "            action = actor_dist.sample()\n",
    "            action = torch.clamp(action, env.action_space.low[0], env.action_space.high[0])\n",
    "            log_prob = actor_dist.log_prob(action).sum()\n",
    "\n",
    "            old_obs = obs\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = 0 if terminated or truncated else critic(obs)\n",
    "            td_error = reward + discount * next_value - critic(old_obs)\n",
    "            td_error = torch.clamp(td_error, -5, 5)\n",
    "            total_reward += reward\n",
    "\n",
    "            critic.zero_grad()\n",
    "            critic(old_obs).sum().backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for trace, param in zip(critic_traces, critic.parameters()):\n",
    "                    trace.mul_(lambda_et * discount).add_(param.grad)\n",
    "                    param.data.add_(lr_weight * td_error * trace)\n",
    "\n",
    "            actor.zero_grad()\n",
    "            log_prob.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for trace, param in zip(actor_traces, actor.parameters()):\n",
    "                    trace.mul_(lambda_et * discount).add_(param.grad)\n",
    "                    param.data.add_(lr_theta * td_error * trace)\n",
    "\n",
    "        lr_theta *= discount_lr\n",
    "        lr_weight *= discount_lr\n",
    "\n",
    "        print(f\"Epoch: {ep} | Total reward: {total_reward} | LR: {lr_theta} | Terminated: {terminated} | Truncated: {truncated}\")\n",
    "\n",
    "def testing_loop_ACWET(epochs):\n",
    "    global actor, critic\n",
    "\n",
    "    env = gym.make(\"InvertedDoublePendulum-v5\", render_mode=\"human\")\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        terminated = truncated = False\n",
    "        obs, _ = env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            actor_dist = actor.get_dist(obs)\n",
    "            action = actor_dist.sample()\n",
    "            action = torch.clamp(action, env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward+=reward\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "            #i_t *= discount\n",
    "\n",
    "        print(f\"Epoch: {ep} | Total reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c79851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_ACWET(10000, 1e-4, 1e-4, 0.95, 0.85, 0.9994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "527dfcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Total reward: 9349.855866087582 | Terminated: False | Truncated: True\n",
      "Epoch: 1 | Total reward: 9348.039792073263 | Terminated: False | Truncated: True\n",
      "Epoch: 2 | Total reward: 9223.858498845268 | Terminated: True | Truncated: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitor/Documentos/reinforcement-learning/venv/lib/python3.13/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Total reward: 8933.315757387216 | Terminated: True | Truncated: False\n",
      "Epoch: 4 | Total reward: 9348.827164725093 | Terminated: False | Truncated: True\n",
      "Epoch: 5 | Total reward: 8146.833796840885 | Terminated: True | Truncated: False\n",
      "Epoch: 6 | Total reward: 4925.604865934828 | Terminated: True | Truncated: False\n",
      "Epoch: 7 | Total reward: 5934.471574451457 | Terminated: True | Truncated: False\n",
      "Epoch: 8 | Total reward: 9351.579689106346 | Terminated: False | Truncated: True\n",
      "Epoch: 9 | Total reward: 866.4769396873893 | Terminated: True | Truncated: False\n"
     ]
    }
   ],
   "source": [
    "testing_loop_ACWET(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
